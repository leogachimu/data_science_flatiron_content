{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Logistic Regression From Scratch - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, you'll practice your ability to translate mathematical algorithms into Python functions. This will deepen and solidify your understanding of logistic regression!\n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this lab you will: \n",
    "\n",
    "- Build a logistic regression model from scratch using gradient descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Recall that the logistic regression algorithm builds upon the intuition from linear regression. In logistic regression, you start by taking the input data, `X`, and multiplying it by a vector of weights for each of the individual features, which produces an output, `y`. Afterward, you'll work on using an iterative approach via gradient descent to tune these weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression setup\n",
    "\n",
    "Write a simple function `predict_y()` that takes in a matrix `X` of observations and a vector of feature weights `w` and outputs a vector of predictions for the various observations.\n",
    "\n",
    "Recall that this is the sum of the product of each of the feature observations and their corresponding feature weights:  \n",
    "  \n",
    "$\\large \\hat{y}_i = X_{i1} \\cdot w_1 + X_{i2} \\cdot w_2 + X_{i3} \\cdot w_3 + ... + X_{in} \\cdot w_n$\n",
    "\n",
    "> **Hint**: Think about which mathematical operation you've seen previously that will take a matrix (`X`) and multiply it by a vector of weights (`w`). Use NumPy! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "import numpy as np\n",
    "\n",
    "def predict_y(X, w): \n",
    "    return np.dot(X, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The sigmoid function\n",
    "\n",
    "Recall that the sigmoid function is used to map the linear regression model output to a range of 0 to 1, satisfying basic premises of probability. As a reminder, the sigmoid function is defined by:  \n",
    "  \n",
    "$S(x) = \\dfrac{1}{1+e^(-x)}$   \n",
    "  \n",
    "Write this as a Python function where `x` is the input and the function outputs the result of the sigmoid function. \n",
    "\n",
    "> **Hint**: Use NumPy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the sigmoid\n",
    "\n",
    "For good measure, let's do a brief investigation of your new function. Plot the output of your `sigmoid()` function using 10,000 values evenly spaced from -20 to 20. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Plot sigmoid\n",
    "x = np.linspace(-20, 20, 10000)\n",
    "plt.plot(x, sigmoid(x))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('sigmoid function');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent with the sigmoid function\n",
    "\n",
    "Recall that gradient descent is a numerical method for finding a minimum to a cost function. In the case of logistic regression, you are looking to minimize the error between the model's predictions and the actual data labels. To do this, you first calculate an error vector based on the current model's feature weights. You then multiply the transpose of the training matrix itself by this error vector in order to obtain the gradient. Finally, you take the gradient, multiply it by the step size and add this to our current weight vector to update it. Below, write such a function. It will take 5 inputs:  \n",
    "\n",
    "* `X`  \n",
    "* `y`   \n",
    "* `max_iterations`   \n",
    "* `alpha` (the step size)   \n",
    "* `initial_weights`   \n",
    "\n",
    "\n",
    "By default, have your function set the `initial_weights` parameter to a vector where all feature weights are set to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "def grad_desc(X, y, max_iterations, alpha, initial_weights=np.ones((13, 303))):\n",
    "    \"\"\"Be sure to set default behavior for the initial_weights parameter.\"\"\"\n",
    "    # Create a for loop of iterations\n",
    "    for iteration in range(max_iterations):\n",
    "        # Generate predictions using the current feature weights\n",
    "        y_pred = predict_y(X, initial_weights)\n",
    "        # Calculate an error vector based on these initial predictions and the correct labels\n",
    "        error_vector = y - y_pred\n",
    "        # Calculate the gradient\n",
    "        # As we saw in the previous lab, calculating the gradient is often the most difficult task.\n",
    "        # Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\n",
    "        # For more details on the derivation, see the additional resources section below.\n",
    "        gradient = np.dot(X.transpose(), error_vector) \n",
    "        # Update the weight vector take a step of alpha in direction of gradient\n",
    "        initial_weights -= alpha * gradient\n",
    "    # Return finalized weights\n",
    "    return initial_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data must be 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgrad_desc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [31], line 9\u001b[0m, in \u001b[0;36mgrad_desc\u001b[0;34m(X, y, max_iterations, alpha, initial_weights)\u001b[0m\n\u001b[1;32m      7\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m predict_y(X, initial_weights)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Calculate an error vector based on these initial predictions and the correct labels\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m error_vector \u001b[38;5;241m=\u001b[39m \u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Calculate the gradient\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# As we saw in the previous lab, calculating the gradient is often the most difficult task.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Here, your are provided with the closed form solution for the gradient of the log-loss function derived from MLE\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# For more details on the derivation, see the additional resources section below.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m gradient \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(X\u001b[38;5;241m.\u001b[39mtranspose(), error_vector) \n",
      "File \u001b[0;32m/opt/saturncloud/envs/saturn/lib/python3.10/site-packages/pandas/core/ops/common.py:72\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     70\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/saturncloud/envs/saturn/lib/python3.10/site-packages/pandas/core/arraylike.py:110\u001b[0m, in \u001b[0;36mOpsMixin.__sub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sub__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__sub__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/saturncloud/envs/saturn/lib/python3.10/site-packages/pandas/core/series.py:6259\u001b[0m, in \u001b[0;36mSeries._arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6257\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_arith_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, op):\n\u001b[1;32m   6258\u001b[0m     \u001b[38;5;28mself\u001b[39m, other \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39malign_method_SERIES(\u001b[38;5;28mself\u001b[39m, other)\n\u001b[0;32m-> 6259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIndexOpsMixin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/saturncloud/envs/saturn/lib/python3.10/site-packages/pandas/core/base.py:1327\u001b[0m, in \u001b[0;36mIndexOpsMixin._arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1325\u001b[0m     result \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39marithmetic_op(lvalues, rvalues, op)\n\u001b[0;32m-> 1327\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_construct_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mres_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/saturncloud/envs/saturn/lib/python3.10/site-packages/pandas/core/series.py:3223\u001b[0m, in \u001b[0;36mSeries._construct_result\u001b[0;34m(self, result, name)\u001b[0m\n\u001b[1;32m   3219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (res1, res2)\n\u001b[1;32m   3221\u001b[0m \u001b[38;5;66;03m# We do not pass dtype to ensure that the Series constructor\u001b[39;00m\n\u001b[1;32m   3222\u001b[0m \u001b[38;5;66;03m#  does inference in the case where `result` has object-dtype.\u001b[39;00m\n\u001b[0;32m-> 3223\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_constructor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3224\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   3226\u001b[0m \u001b[38;5;66;03m# Set the result's name after __finalize__ is called because __finalize__\u001b[39;00m\n\u001b[1;32m   3227\u001b[0m \u001b[38;5;66;03m#  would set it back to self.name\u001b[39;00m\n",
      "File \u001b[0;32m/opt/saturncloud/envs/saturn/lib/python3.10/site-packages/pandas/core/series.py:470\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    468\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 470\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43msanitize_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m     manager \u001b[38;5;241m=\u001b[39m get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.data_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/saturncloud/envs/saturn/lib/python3.10/site-packages/pandas/core/construction.py:647\u001b[0m, in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure, allow_2d)\u001b[0m\n\u001b[1;32m    644\u001b[0m             subarr \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mndarray, subarr)\n\u001b[1;32m    645\u001b[0m             subarr \u001b[38;5;241m=\u001b[39m maybe_infer_to_datetimelike(subarr)\n\u001b[0;32m--> 647\u001b[0m subarr \u001b[38;5;241m=\u001b[39m \u001b[43m_sanitize_ndim\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_2d\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(subarr, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;66;03m# at this point we should have dtype be None or subarr.dtype == dtype\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mdtype, dtype)\n",
      "File \u001b[0;32m/opt/saturncloud/envs/saturn/lib/python3.10/site-packages/pandas/core/construction.py:698\u001b[0m, in \u001b[0;36m_sanitize_ndim\u001b[0;34m(result, data, dtype, index, allow_2d)\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m allow_2d:\n\u001b[1;32m    697\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m--> 698\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData must be 1-dimensional\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_object_dtype(dtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype):\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;66;03m# i.e. PandasDtype(\"O\")\u001b[39;00m\n\u001b[1;32m    702\u001b[0m     result \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39masarray_tuplesafe(data, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mValueError\u001b[0m: Data must be 1-dimensional"
     ]
    }
   ],
   "source": [
    "grad_desc(X, y, 100, 1*10**(-7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((303, 13))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running your algorithm\n",
    "\n",
    "Now that you've coded everything from the ground up, you can further investigate the convergence behavior of the gradient descent algorithm. Remember that gradient descent does not guarantee a global minimum, only a local minimum, and that small deviations in the starting point or step size can lead to different outputs.  \n",
    "  \n",
    "First, run the following cell to import the data and create the predictor and target variables: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0    165\n",
      "0.0    138\n",
      "Name: target, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.708333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.481132</td>\n",
       "      <td>0.244292</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.603053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.370968</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.283105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.885496</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.564516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.178082</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.770992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.562500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.251142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.816794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.520548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.702290</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age  sex        cp  trestbps      chol  fbs  restecg   thalach  exang  \\\n",
       "0  0.708333  1.0  1.000000  0.481132  0.244292  1.0      0.0  0.603053    0.0   \n",
       "1  0.166667  1.0  0.666667  0.339623  0.283105  0.0      0.5  0.885496    0.0   \n",
       "2  0.250000  0.0  0.333333  0.339623  0.178082  0.0      0.0  0.770992    0.0   \n",
       "3  0.562500  1.0  0.333333  0.245283  0.251142  0.0      0.5  0.816794    0.0   \n",
       "4  0.583333  0.0  0.000000  0.245283  0.520548  0.0      0.5  0.702290    1.0   \n",
       "\n",
       "    oldpeak  slope   ca      thal  \n",
       "0  0.370968    0.0  0.0  0.333333  \n",
       "1  0.564516    0.0  0.0  0.666667  \n",
       "2  0.225806    1.0  0.0  0.666667  \n",
       "3  0.129032    1.0  0.0  0.666667  \n",
       "4  0.096774    1.0  0.0  0.666667  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data\n",
    "import pandas as pd\n",
    "df = pd.read_csv('heart.csv')\n",
    "\n",
    "# Create the predictor and target variables\n",
    "y = df['target']\n",
    "X = df.drop(columns=['target'], axis=1)\n",
    "\n",
    "print(y.value_counts())\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run your algorithm and plot the successive weights of the features through iterations. Below is a dataset, with `X` and `y` predefined for you. Use your logistic regression function to train a model. As the model trains, record the iteration cycle of the gradient descent algorithm and the weights of the various features. Then, plot this data on subplots for each of the individual features. Each graph should have the iteration number on the x-axis and the value of that feature weight for that iteration cycle on the y-axis. This will visually display how the algorithm is adjusting the weights over successive iterations, and hopefully show convergence to stable weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn\n",
    "\n",
    "For comparison, import scikit-learn's standard `LogisticRegression()` function. Initialize it with **no intercept** and **C=1e16** or another very high number. The reason is as follows: our implementation has not used an intercept, and you have not performed any regularization such as Lasso or Ridge (scikit-learn uses l2 by default). The high value of `C` will essentially negate this. Also, set the `random_state` to 2 and use the `'liblinear'` solver. \n",
    "\n",
    "After initializing a regression object, fit it to `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=1e+16, fit_intercept=False, random_state=2,\n",
       "                   solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=1e+16, fit_intercept=False, random_state=2,\n",
       "                   solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=1e+16, fit_intercept=False, random_state=2,\n",
       "                   solver='liblinear')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(C=1e16, fit_intercept=False, random_state=2, solver='liblinear')\n",
    "logreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the models\n",
    "\n",
    "Compare the coefficient weights of your model to that generated by scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.81221467 -1.61293693  2.6179496  -1.96887354 -1.50936862  0.05688225\n",
      "   1.1521945   4.42098698 -0.83027951 -2.74686109  1.45580368 -3.115599\n",
      "  -2.19130405]]\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "print(logreg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level up (Optional)\n",
    "\n",
    "Update the gradient descent algorithm to also return the cost after each iteration. Then rerun the algorithm and create a graph displaying the cost versus the iteration number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "If you want to see more of the mathematics behind the gradient derivation above, check out section 4.4.1 from the Elements of Statistical Learning which can be found here: https://web.stanford.edu/~hastie/ElemStatLearn//."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You just coded logistic regression from the ground up using NumPy! With this, you should have a fairly deep understanding of logistic regression and how the algorithm works! In the upcoming labs, you'll continue to explore this from a few more angles, plotting your data along with the decision boundary for our predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
